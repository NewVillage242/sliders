{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8fa4f8a",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rohitgandikota/sliders/blob/main/demo_concept_sliders.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bb3fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import os, json, random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, re\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "import matplotlib.image as mpimg\n",
    "import copy\n",
    "import gc\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "import diffusers\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, UNet2DConditionModel, LMSDiscreteScheduler\n",
    "from diffusers.loaders import AttnProcsLayers\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor, AttentionProcessor\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from trainscripts.textsliders.lora import LoRANetwork, DEFAULT_TARGET_REPLACE, UNET_TARGET_REPLACE_MODULE_CONV\n",
    "from diffusers.pipelines.stable_diffusion_xl import StableDiffusionXLPipelineOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08aec8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my implement of encode_prompt\n",
    "from diffusers.loaders import FromSingleFileMixin, LoraLoaderMixin, TextualInversionLoaderMixin\n",
    "from diffusers.utils import (\n",
    "    is_accelerate_available,\n",
    "    is_accelerate_version,\n",
    "    is_invisible_watermark_available,\n",
    "    logging,\n",
    "    randn_tensor,\n",
    "    replace_example_docstring,\n",
    ")\n",
    "logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n",
    "\n",
    "def my_encode_prompt(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        prompt_2: Optional[str] = None,\n",
    "        device: Optional[torch.device] = None,\n",
    "        num_images_per_prompt: int = 1,\n",
    "        do_classifier_free_guidance: bool = True,\n",
    "        negative_prompt: Optional[str] = None,\n",
    "        negative_prompt_2: Optional[str] = None,\n",
    "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        lora_scale: Optional[float] = None,\n",
    "        text_encoder=None\n",
    "    ):\n",
    "        device = device or self._execution_device\n",
    "\n",
    "        # set lora scale so that monkey patched LoRA\n",
    "        # function of text encoder can correctly access it\n",
    "        \n",
    "        # My implementation\n",
    "        print(f\"my_encoder_prompt loar_scale : {lora_scale}\")\n",
    "        self.text_encoder = text_encoder if text_encoder is not None else self.text_encoder\n",
    "        \n",
    "        \n",
    "        if lora_scale is not None and isinstance(self, LoraLoaderMixin):\n",
    "            self._lora_scale = lora_scale\n",
    "\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "        # Define tokenizers and text encoders\n",
    "        tokenizers = [self.tokenizer, self.tokenizer_2] if self.tokenizer is not None else [self.tokenizer_2]\n",
    "        text_encoders = (\n",
    "            [self.text_encoder, self.text_encoder_2] if self.text_encoder is not None else [self.text_encoder_2]\n",
    "        )\n",
    "\n",
    "        if prompt_embeds is None:\n",
    "            prompt_2 = prompt_2 or prompt\n",
    "            # textual inversion: procecss multi-vector tokens if necessary\n",
    "            prompt_embeds_list = []\n",
    "            prompts = [prompt, prompt_2]\n",
    "            for prompt, tokenizer, text_encoder in zip(prompts, tokenizers, text_encoders):\n",
    "                if isinstance(self, TextualInversionLoaderMixin):\n",
    "                    prompt = self.maybe_convert_prompt(prompt, tokenizer)\n",
    "\n",
    "                text_inputs = tokenizer(\n",
    "                    prompt,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=tokenizer.model_max_length,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "\n",
    "                text_input_ids = text_inputs.input_ids\n",
    "                untruncated_ids = tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "                if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(\n",
    "                    text_input_ids, untruncated_ids\n",
    "                ):\n",
    "                    removed_text = tokenizer.batch_decode(untruncated_ids[:, tokenizer.model_max_length - 1 : -1])\n",
    "                    logger.warning(\n",
    "                        \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n",
    "                        f\" {tokenizer.model_max_length} tokens: {removed_text}\"\n",
    "                    )\n",
    "\n",
    "                prompt_embeds = text_encoder(\n",
    "                    text_input_ids.to(device),\n",
    "                    output_hidden_states=True,\n",
    "                )\n",
    "\n",
    "                # We are only ALWAYS interested in the pooled output of the final text encoder\n",
    "                pooled_prompt_embeds = prompt_embeds[0]\n",
    "                prompt_embeds = prompt_embeds.hidden_states[-2]\n",
    "\n",
    "                prompt_embeds_list.append(prompt_embeds)\n",
    "\n",
    "            prompt_embeds = torch.concat(prompt_embeds_list, dim=-1)\n",
    "\n",
    "        # get unconditional embeddings for classifier free guidance\n",
    "        zero_out_negative_prompt = negative_prompt is None and self.config.force_zeros_for_empty_prompt\n",
    "        if do_classifier_free_guidance and negative_prompt_embeds is None and zero_out_negative_prompt:\n",
    "            negative_prompt_embeds = torch.zeros_like(prompt_embeds)\n",
    "            negative_pooled_prompt_embeds = torch.zeros_like(pooled_prompt_embeds)\n",
    "        elif do_classifier_free_guidance and negative_prompt_embeds is None:\n",
    "            negative_prompt = negative_prompt or \"\"\n",
    "            negative_prompt_2 = negative_prompt_2 or negative_prompt\n",
    "\n",
    "            uncond_tokens: List[str]\n",
    "            if prompt is not None and type(prompt) is not type(negative_prompt):\n",
    "                raise TypeError(\n",
    "                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n",
    "                    f\" {type(prompt)}.\"\n",
    "                )\n",
    "            elif isinstance(negative_prompt, str):\n",
    "                uncond_tokens = [negative_prompt, negative_prompt_2]\n",
    "            elif batch_size != len(negative_prompt):\n",
    "                raise ValueError(\n",
    "                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n",
    "                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n",
    "                    \" the batch size of `prompt`.\"\n",
    "                )\n",
    "            else:\n",
    "                uncond_tokens = [negative_prompt, negative_prompt_2]\n",
    "\n",
    "            negative_prompt_embeds_list = []\n",
    "            for negative_prompt, tokenizer, text_encoder in zip(uncond_tokens, tokenizers, text_encoders):\n",
    "                if isinstance(self, TextualInversionLoaderMixin):\n",
    "                    negative_prompt = self.maybe_convert_prompt(negative_prompt, tokenizer)\n",
    "\n",
    "                max_length = prompt_embeds.shape[1]\n",
    "                uncond_input = tokenizer(\n",
    "                    negative_prompt,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=max_length,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "\n",
    "                negative_prompt_embeds = text_encoder(\n",
    "                    uncond_input.input_ids.to(device),\n",
    "                    output_hidden_states=True,\n",
    "                )\n",
    "                # We are only ALWAYS interested in the pooled output of the final text encoder\n",
    "                negative_pooled_prompt_embeds = negative_prompt_embeds[0]\n",
    "                negative_prompt_embeds = negative_prompt_embeds.hidden_states[-2]\n",
    "\n",
    "                negative_prompt_embeds_list.append(negative_prompt_embeds)\n",
    "\n",
    "            negative_prompt_embeds = torch.concat(negative_prompt_embeds_list, dim=-1)\n",
    "\n",
    "        prompt_embeds = prompt_embeds.to(dtype=self.text_encoder_2.dtype, device=device)\n",
    "        bs_embed, seq_len, _ = prompt_embeds.shape\n",
    "        # duplicate text embeddings for each generation per prompt, using mps friendly method\n",
    "        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "        prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "        if do_classifier_free_guidance:\n",
    "            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n",
    "            seq_len = negative_prompt_embeds.shape[1]\n",
    "            negative_prompt_embeds = negative_prompt_embeds.to(dtype=self.text_encoder_2.dtype, device=device)\n",
    "            negative_prompt_embeds = negative_prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "            negative_prompt_embeds = negative_prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "        pooled_prompt_embeds = pooled_prompt_embeds.repeat(1, num_images_per_prompt).view(\n",
    "            bs_embed * num_images_per_prompt, -1\n",
    "        )\n",
    "        if do_classifier_free_guidance:\n",
    "            negative_pooled_prompt_embeds = negative_pooled_prompt_embeds.repeat(1, num_images_per_prompt).view(\n",
    "                bs_embed * num_images_per_prompt, -1\n",
    "            )\n",
    "\n",
    "        return prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59764dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import os\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "from diffusers.pipelines import StableDiffusionXLPipeline\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTextModelWithProjection, CLIPTokenizer\n",
    "\n",
    "def flush():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "@torch.no_grad()\n",
    "def call(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]] = None,\n",
    "        prompt_2: Optional[Union[str, List[str]]] = None,\n",
    "        height: Optional[int] = None,\n",
    "        width: Optional[int] = None,\n",
    "        num_inference_steps: int = 50,\n",
    "        denoising_end: Optional[float] = None,\n",
    "        guidance_scale: float = 5.0,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        negative_prompt_2: Optional[Union[str, List[str]]] = None,\n",
    "        num_images_per_prompt: Optional[int] = 1,\n",
    "        eta: float = 0.0,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        latents: Optional[torch.FloatTensor] = None,\n",
    "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
    "        callback_steps: int = 1,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        guidance_rescale: float = 0.0,\n",
    "        original_size: Optional[Tuple[int, int]] = None,\n",
    "        crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
    "        target_size: Optional[Tuple[int, int]] = None,\n",
    "        negative_original_size: Optional[Tuple[int, int]] = None,\n",
    "        negative_crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
    "        negative_target_size: Optional[Tuple[int, int]] = None,\n",
    "    \n",
    "        network:LoRANetwork=None, \n",
    "        start_noise=None,\n",
    "        scale=None,\n",
    "        unet=None,\n",
    "        text_encoder=None,\n",
    "    ):\n",
    "        # 0. Default height and width to unet\n",
    "        height = height or self.default_sample_size * self.vae_scale_factor\n",
    "        width = width or self.default_sample_size * self.vae_scale_factor\n",
    "\n",
    "        original_size = original_size or (height, width)\n",
    "        target_size = target_size or (height, width)\n",
    "\n",
    "        # 1. Check inputs. Raise error if not correct\n",
    "        self.check_inputs(\n",
    "            prompt,\n",
    "            prompt_2,\n",
    "            height,\n",
    "            width,\n",
    "            callback_steps,\n",
    "            negative_prompt,\n",
    "            negative_prompt_2,\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds,\n",
    "        )\n",
    "\n",
    "        # 2. Define call parameters\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "        device = self._execution_device\n",
    "\n",
    "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
    "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
    "        # corresponds to doing no classifier free guidance.\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "\n",
    "        # 3. Encode input prompt\n",
    "        # my implement\n",
    "        self.encode_prompt = my_encode_prompt\n",
    "        network.set_lora_slider(scale=scale)\n",
    "        with network:\n",
    "            (\n",
    "                prompt_embeds,\n",
    "                negative_prompt_embeds,\n",
    "                pooled_prompt_embeds,\n",
    "                negative_pooled_prompt_embeds,\n",
    "            ) = self.encode_prompt(\n",
    "                self=self,\n",
    "                prompt=prompt,\n",
    "                prompt_2=prompt_2,\n",
    "                device=device,\n",
    "                num_images_per_prompt=num_images_per_prompt,\n",
    "                do_classifier_free_guidance=do_classifier_free_guidance,\n",
    "                negative_prompt=negative_prompt,\n",
    "                negative_prompt_2=negative_prompt_2,\n",
    "                prompt_embeds=prompt_embeds,\n",
    "                negative_prompt_embeds=negative_prompt_embeds,\n",
    "                pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "                negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "                lora_scale=scale,\n",
    "                text_encoder=text_encoder\n",
    "            )\n",
    "        \n",
    "        # 4. Prepare timesteps\n",
    "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "\n",
    "        timesteps = self.scheduler.timesteps\n",
    "\n",
    "        # 5. Prepare latent variables\n",
    "        num_channels_latents = unet.config.in_channels\n",
    "        latents = self.prepare_latents(\n",
    "            batch_size * num_images_per_prompt,\n",
    "            num_channels_latents,\n",
    "            height,\n",
    "            width,\n",
    "            prompt_embeds.dtype,\n",
    "            device,\n",
    "            generator,\n",
    "            latents,\n",
    "        )\n",
    "\n",
    "        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
    "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "\n",
    "        # 7. Prepare added time ids & embeddings\n",
    "        add_text_embeds = pooled_prompt_embeds\n",
    "        add_time_ids = self._get_add_time_ids(\n",
    "            original_size, crops_coords_top_left, target_size, dtype=prompt_embeds.dtype\n",
    "        )\n",
    "        if negative_original_size is not None and negative_target_size is not None:\n",
    "            negative_add_time_ids = self._get_add_time_ids(\n",
    "                negative_original_size,\n",
    "                negative_crops_coords_top_left,\n",
    "                negative_target_size,\n",
    "                dtype=prompt_embeds.dtype,\n",
    "            )\n",
    "        else:\n",
    "            negative_add_time_ids = add_time_ids\n",
    "\n",
    "        if do_classifier_free_guidance:\n",
    "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n",
    "            add_text_embeds = torch.cat([negative_pooled_prompt_embeds, add_text_embeds], dim=0)\n",
    "            add_time_ids = torch.cat([negative_add_time_ids, add_time_ids], dim=0)\n",
    "\n",
    "        prompt_embeds = prompt_embeds.to(device)\n",
    "        add_text_embeds = add_text_embeds.to(device)\n",
    "        add_time_ids = add_time_ids.to(device).repeat(batch_size * num_images_per_prompt, 1)\n",
    "\n",
    "        # 8. Denoising loop\n",
    "        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n",
    "\n",
    "        # 7.1 Apply denoising_end\n",
    "        if denoising_end is not None and isinstance(denoising_end, float) and denoising_end > 0 and denoising_end < 1:\n",
    "            discrete_timestep_cutoff = int(\n",
    "                round(\n",
    "                    self.scheduler.config.num_train_timesteps\n",
    "                    - (denoising_end * self.scheduler.config.num_train_timesteps)\n",
    "                )\n",
    "            )\n",
    "            num_inference_steps = len(list(filter(lambda ts: ts >= discrete_timestep_cutoff, timesteps)))\n",
    "            timesteps = timesteps[:num_inference_steps]\n",
    "        latents = latents.to(unet.dtype)\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                if t>start_noise:\n",
    "                    network.set_lora_slider(scale=0)\n",
    "                else:\n",
    "                    network.set_lora_slider(scale=scale)\n",
    "                # expand the latents if we are doing classifier free guidance\n",
    "                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "\n",
    "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                # predict the noise residual\n",
    "                added_cond_kwargs = {\"text_embeds\": add_text_embeds, \"time_ids\": add_time_ids}\n",
    "                with network:\n",
    "                    noise_pred = unet(\n",
    "                        latent_model_input,\n",
    "                        t,\n",
    "                        encoder_hidden_states=prompt_embeds,\n",
    "                        cross_attention_kwargs=cross_attention_kwargs,\n",
    "                        added_cond_kwargs=added_cond_kwargs,\n",
    "                        return_dict=False,\n",
    "                    )[0]\n",
    "\n",
    "                # perform guidance\n",
    "                if do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                if do_classifier_free_guidance and guidance_rescale > 0.0:\n",
    "                    # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf\n",
    "                    noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=guidance_rescale)\n",
    "\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n",
    "\n",
    "                # call the callback, if provided\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "                    if callback is not None and i % callback_steps == 0:\n",
    "                        callback(i, t, latents)\n",
    "\n",
    "        if not output_type == \"latent\":\n",
    "            # make sure the VAE is in float32 mode, as it overflows in float16\n",
    "            needs_upcasting = self.vae.dtype == torch.float16 and self.vae.config.force_upcast\n",
    "\n",
    "            if needs_upcasting:\n",
    "                self.upcast_vae()\n",
    "                latents = latents.to(next(iter(self.vae.post_quant_conv.parameters())).dtype)\n",
    "\n",
    "            image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n",
    "\n",
    "            # cast back to fp16 if needed\n",
    "            if needs_upcasting:\n",
    "                self.vae.to(dtype=torch.float16)\n",
    "        else:\n",
    "            image = latents\n",
    "\n",
    "        if not output_type == \"latent\":\n",
    "            # apply watermark if available\n",
    "            if self.watermark is not None:\n",
    "                image = self.watermark.apply_watermark(image)\n",
    "\n",
    "            image = self.image_processor.postprocess(image, output_type=output_type)\n",
    "\n",
    "        # Offload all models\n",
    "#         self.maybe_free_model_hooks()\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image,)\n",
    "\n",
    "        return StableDiffusionXLPipelineOutput(images=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cac1968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70dab8e1e53e44aca00004cbb1a3205a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "StableDiffusionXLPipeline.__call__ = call\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained('stabilityai/stable-diffusion-xl-base-1.0')\n",
    "\n",
    "# pipe.__call__ = call\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5c15f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_weights =[\n",
    "    'models/gender_slider_alphaNone_rank4_noxattn_clip/gender_slider_alphaNone_rank4_noxattn_clip_last.pt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f5d3fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'Image of a person, realistic, 8k',\n",
    "#     'A realistic picture of a person',\n",
    "#     'A realistic photograph of a person, bokeh, blurred background, 8k',\n",
    "#     'Professional headshot of a person',\n",
    "#     'closeup photo of a person on city street, realistic, 8k'\n",
    "#     \"Astronaut in a jungle holding an umbrella, cold color palette, muted colors, detailed, 8k\",\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4044a649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image of a person, realistic, 8k 19110\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2570e4e3f7943dfa0ff6c934a6d52fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRANetwork\n",
      "create LoRA for Clip: 24 modules.\n",
      "1.0 0 0\n",
      "1.0 0.9 0.9\n",
      "tensor([[[-0.3887,  0.0228, -0.0523,  ..., -0.4900, -0.3066,  0.0673],\n",
      "         [ 0.5103,  0.4756,  1.8076,  ..., -0.6445,  0.9253,  0.1665],\n",
      "         [-0.0622,  0.6538,  1.4238,  ..., -0.5635, -0.3806, -0.5776],\n",
      "         ...,\n",
      "         [ 2.0801, -0.5659,  0.6357,  ..., -1.1152,  0.1385, -0.7588],\n",
      "         [ 2.1016, -0.5737,  0.6372,  ..., -1.1172,  0.1757, -0.7671],\n",
      "         [ 1.9521, -0.5205,  0.6548,  ..., -1.2031,  0.1766, -0.6860]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-0.3887,  0.0228, -0.0523,  ..., -0.4900, -0.3066,  0.0673],\n",
      "         [ 0.5103,  0.4756,  1.8076,  ..., -0.6445,  0.9253,  0.1665],\n",
      "         [-0.0622,  0.6538,  1.4238,  ..., -0.5635, -0.3806, -0.5776],\n",
      "         ...,\n",
      "         [ 2.0801, -0.5659,  0.6357,  ..., -1.1152,  0.1385, -0.7588],\n",
      "         [ 2.1016, -0.5737,  0.6372,  ..., -1.1172,  0.1757, -0.7671],\n",
      "         [ 1.9521, -0.5205,  0.6548,  ..., -1.2031,  0.1766, -0.6860]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "weight_dtype = torch.float16\n",
    "start_noise = 700\n",
    "num_images_per_prompt = 1\n",
    "scales = [0, 0.9]\n",
    "prompt=prompts[-1]\n",
    "seed = 19110\n",
    "print(prompt, seed)\n",
    "lora_weight = lora_weights[-1]\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained('stabilityai/stable-diffusion-xl-base-1.0', torch_dtype=weight_dtype)\n",
    "\n",
    "# pipe.__call__ = call\n",
    "pipe = pipe.to(device)\n",
    "unet = pipe.unet\n",
    "if 'full' in lora_weight:\n",
    "    train_method = 'full'\n",
    "elif 'noxattn' in lora_weight:\n",
    "    train_method = 'noxattn'\n",
    "else:\n",
    "    train_method = 'noxattn'\n",
    "\n",
    "network_type = \"c3lier\"\n",
    "if train_method == 'xattn':\n",
    "    network_type = 'lierla'\n",
    "\n",
    "TEXT_ENCODER_TARGET_MODULES = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "modules = TEXT_ENCODER_TARGET_MODULES\n",
    "if network_type == \"c3lier\":\n",
    "    modules += UNET_TARGET_REPLACE_MODULE_CONV\n",
    "import os\n",
    "model_name = lora_weight\n",
    "\n",
    "name = os.path.basename(model_name)\n",
    "rank = 4\n",
    "alpha = 1\n",
    "\n",
    "network:LoRANetwork = LoRANetwork(\n",
    "        pipe.text_encoder,\n",
    "        rank=rank,\n",
    "        multiplier=1.0,\n",
    "        alpha=alpha,\n",
    "        train_method=train_method,\n",
    "        lora_type=\"clip\",\n",
    "    ).to(device, dtype=weight_dtype)\n",
    "# This cause error.\n",
    "network.load_state_dict(torch.load(lora_weight))\n",
    "\n",
    "\n",
    "image_list = []\n",
    "embed_list=[]\n",
    "for scale in scales:\n",
    "    generator = torch.manual_seed(seed)\n",
    "    network.set_lora_slider(scale)\n",
    "    with network:\n",
    "        print (network.multiplier, network.lora_scale, scale)\n",
    "        text_inputs = pipe.tokenizer(\n",
    "                    prompt,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=pipe.tokenizer.model_max_length,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "\n",
    "        text_input_ids = text_inputs.input_ids.to(device)\n",
    "        embed_list.append(pipe.text_encoder(text_input_ids))\n",
    "\n",
    "\n",
    "print(embed_list[0][0])\n",
    "print(embed_list[1][0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
